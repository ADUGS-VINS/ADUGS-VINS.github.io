<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ADUGS-VINS: Generalized Visual-Inertial Odometry for Robust Navigation in Highly Dynamic and Complex Environments</title>
  <link rel="icon" type="image/x-icon" href="static/images/wuhan-university.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ADUGS-VINS: Generalized Visual-Inertial Odometry for Robust Navigation in Highly Dynamic and Complex Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Rui Zhou</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jingbin Liu</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Junbin Xie</a>,</span>
                    <span class="author-block">
                      <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jianyu Zhang</a>,</span>
                      <span class="author-block">
                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yingze Hu</a>,</span>
                        <span class="author-block">
                          <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jiele Zhao</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Wuhan University<br>Arixv preprint</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.19289.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-dataset"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.19289" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="125%">
        <!-- Your video here -->
        <source src="static/videos/intro.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual-inertial odometry (VIO) is widely used in various fields, such as robots, drones, and autonomous vehicles. However, real-world scenes often feature dynamic objects, compromising the accuracy of VIO. The diversity and partial occlusion of these objects present a tough challenge for existing dynamic VIO methods. To tackle this challenge, we introduce ADUGS-VINS, which integrates an enhanced SORT algorithm along with a promptable foundation model into VIO, thereby improving pose estimation accuracy in environments with diverse dynamic objects and frequent occlusions. We evaluated our proposed method using multiple public datasets representing various scenes, as well as in a real-world scenario involving diverse dynamic objects. The experimental results demonstrate that our proposed method performs impressively in multiple scenarios, outperforming other state-of-the-art methods. This highlights its remarkable generalization and adaptability in diverse dynamic environments, showcasing its potential to handle various dynamic objects in practical applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <div class="columns is-multiline">
        <div class="container">
          <p style="font-size: 28px;">ADUGS-VINS effectively segments various moving objects even under conditions of partial occlusion, demonstrating the efficacy of ADUGS-VINS in segmentation within complex environments.</p>
        </div>
        <div class="column is-full">
          <img src="static/images/CNN_combine_SAM.png" alt="First image description" class="image">
          <h2 class="subtitle has-text-centered">Overview of ADUGS-VINS. </h2>
        </div>

        <div class="columns is-multiline">

          <div class="column is-one-third">
            <img src="static/images/parking_lot_final.jpg" alt="First image description" class="image">
            <h2 class="subtitle has-text-centered">VIODE Dataset parking lot environment</h2>
          </div>
  
          <div class="column is-one-third">
            <img src="static/images/city_day_final.jpg" alt="Second image description" class="image">
            <h2 class="subtitle has-text-centered">VIODE Dataset city day environment</h2>
          </div>
  
          <div class="column is-one-third">
            <img src="static/images/city_night_final.jpg" alt="Third image description" class="image">
            <h2 class="subtitle has-text-centered">VIODE Dataset city night environment</h2>
          </div>
  
        </div>

      </div>

    </div>
  </div>
</section>

<!-- End image carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Real-world Experiment and Collected Dataset</h2>

      

      <div class="columns is-multiline is-flex is-justify-content-center">
        <div class="column is-full has-text-centered">
          <video poster="" id="video2" autoplay controls muted loop width="75%">
            <source src="static/videos/viode_parking_lot.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">The estimated trajectories in the real-world environment aligned with the satellite imagery...</h2>
        </div>
      </div>
      

      </div>

    </div>
  </div>
</section>







<!-- Video Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Video Demonstrations</h2>

      <div class="container">
        <p style="font-size: 28px; text-align: center;"> 
          A RealSense D435i camera captures visual and inertial data for monocular visual-inertial SLAM. 
          We collected an extensive outdoor dataset featuring a variety of moving objects, 
          including pedestrians, cars, buses, motorcycles, and tricycles.
        </p>
      </div>

      <!-- 视频 1：实验平台 -->
      <div class="columns is-flex is-justify-content-center">
        <div class="column is-half has-text-centered">
          <video poster="" id="video1" autoplay controls muted loop width="100%">
            <source src="static/videos/platform.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle">Real-world Experiment Platform.</h2>
        </div>
      </div>

      <!-- 数据集下载链接 -->
      <div class="has-text-centered">
        <p style="font-size: 28px;">
          The dataset collected in real-world environments is publicly available at 
          <a href="https://huggingface.co/datasets/zhourui9813/GMS-VINS-Dataset" 
             style="color: blue; font-weight: bold; text-decoration: none;">
            Huggingface
          </a>.
        </p>
        
      </div>

      <!-- 视频 2：轨迹估计 -->
      <div class="columns is-flex is-justify-content-center">
        <div class="column is-half has-text-centered">
          <video poster="" id="video2" autoplay controls muted loop width="100%">
            <source src="static/videos/real_world_traj1.mp4" type="video/mp4">
          </video>
          <h2 class="subtitle">
            The video illustrates the estimated trajectories in the real-world environment aligned with the satellite imagery. 
            The red line represents the estimated trajectory obtained from ADUGS-VINS, 
            while the blue line corresponds to the trajectory generated by VINS-Mono.
          </h2>
        </div>
      </div>

    </div>
  </div>
</section>










<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
